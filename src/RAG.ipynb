{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loring/miniconda3/envs/openai/lib/python3.9/site-packages/scenic/core/errors.py:271: UserWarning: unable to install sys.excepthook to format Scenic backtraces\n",
      "  warnings.warn(\"unable to install sys.excepthook to format Scenic backtraces\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import tiktoken\n",
    "import scenic\n",
    "import random\n",
    "from langchain_openai import ChatOpenAI, OpenAI, OpenAIEmbeddings\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from OutputParser import Scenic_output\n",
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_community.embeddings import LlamaCppEmbeddings\n",
    "# from scenic.simulators.carla import CarlaSimulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Configurations for OpenAI\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-7be80d5f4b73475d9ab5e1e998458cec\"\n",
    "base_url = \"https://api.deepseek.com/v1\"\n",
    "model_name = \"deepseek-chat\"\n",
    "model = ChatOpenAI(\n",
    "    model_name=model_name,\n",
    "    max_tokens=2048,\n",
    "    temperature=0,\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    openai_api_base= base_url\n",
    ")\n",
    "structured_model = model.with_structured_output(Scenic_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful functions\n",
    "###############################################\n",
    "# Calculate the number of tokens in a message.#\n",
    "###############################################\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\", include_final_response_prefix=True):\n",
    "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        #print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if \"gpt-3.5\" in model:\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted                                                                                                                                                                                             \n",
    "    elif \"gpt-4\" in model or \"deepseek\" in model:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md \\\n",
    "        for information on how messages are converted to tokens.\"\"\")\n",
    "    \n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    if include_final_response_prefix:\n",
    "        num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens\n",
    "\n",
    "################################################\n",
    "# Convert the example to chat messages         #\n",
    "################################################\n",
    "# Example are in json formats and messages is in the format of the chat\n",
    "def example_to_chat_messages(example):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"name\": \"example_user\", \n",
    "         \"content\": '\"\"\" Scenario description\\n%s\\n\"\"\"' % example['docstring']},\n",
    "        {\"role\": \"system\", \"name\": \"example_assistant\", \n",
    "         \"content\": '```##Map and Model##\\n%s\\n##Constants##\\n%s\\n##Moniters##\\n%s\\n##Defining Agent Behaviors##\\n%s\\n##Spatial Relations##\\n%s\\n##Scenario Specifications##\\n%s\\n##Background Activities##\\n%s\\n```' \\\n",
    "             % (example['map_and_model'], example['constants'], example['monitors'], example['behaviors'], example['spatial_relations'], example['scenario'], example['background'])}\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "################################################\n",
    "# Get the number of tokens for the example     #\n",
    "################################################\n",
    "# We need to construct the chat messages from the example and then calculate the number of tokens\n",
    "def get_example_num_tokens(example):\n",
    "    return num_tokens_from_messages(example_to_chat_messages(example), model_name, \n",
    "                                    include_final_response_prefix=False)\n",
    "\n",
    "################################################\n",
    "# Convert the input to chat messages           #\n",
    "################################################\n",
    "# Input is a string or a dictionary with docstring\n",
    "# If the input is a sole string, then we should directly add it to Scenario Description\n",
    "# If the input is a dictionary, then we should extract the docstring from it\n",
    "def input_to_chat_messages(example):\n",
    "    docstring = example if (isinstance(example, str)) else example['docstring']\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": '\"\"\" Scenario description\\n%s\\n\"\"\"' % docstring}\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "################################################\n",
    "# Get the number of tokens for the input       #\n",
    "################################################\n",
    "# We need to construct the chat messages from the input and then calculate the number of tokens\n",
    "def get_input_num_tokens(example, include_final_response_prefix=True):\n",
    "    return num_tokens_from_messages(input_to_chat_messages(example), model_name, \n",
    "                                    include_final_response_prefix=include_final_response_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure a formatted request in LLM should contain the following fields:\n",
    "- _System Message_\n",
    "    - This field will determine the role of the message in the system. It will be used to determine the type of message that is being sent.\n",
    "- _Example Message_\n",
    "    - This field will contain the format of the message that is being sent.\n",
    "- _Prompt Message/Input Message_\n",
    "    - This field will contain the prompt message that is being sent to the model. Based on the prompt message, the model will modify some of the field values in the Example Message and generate new scenarios.\n",
    "- _Response Message_\n",
    "    - This field will contain the response message that is generated by the model. This message will be generated based on the Example Message and Prompt Message.\n",
    "- _RAG Status_\n",
    "    - This field will contain the RAG status of the message. This will be used to determine the quality of the response generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167\n"
     ]
    }
   ],
   "source": [
    "# System message information\n",
    "system_message_str=\"\"\"You are a helpful agent that generates specifications for car driving scenarios in the Scenic language\n",
    "Scenic is a domain-specific probabilistic programming language for modeling the environments of cyber-physical systems like robots and autonomous cars. A Scenic program defines a distribution over scenes,\\\n",
    "configurations of physical objects and agents; sampling from this distribution yields concrete scenes which can be simulated to produce training or testing data. Scenic can also define (probabilistic)\\\n",
    "policies for dynamic agents, allowing modeling scenarios where agents take actions over time in response to the state of the world.\\\n",
    "\n",
    "Your task is to generate Scenic scenarios, each according to its corresponding description in English included as a docstring. \\\n",
    "Please refer all the codes to the given few-shot examples and reference guide syntax. Some self-made APIs are not implemented in Scenic language:\"\"\"\n",
    "\n",
    "system_role = \"system\"\n",
    "system_message = {\"role\": system_role, \"content\": system_message_str}\n",
    "system_message_len = num_tokens_from_messages([system_message], model_name, include_final_response_prefix=False)\n",
    "# system_message_len = num_tokens_from_messages([system_message], include_final_response_prefix=False)\n",
    "print(system_message_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example message information\n",
    "import os\n",
    "from JsonFormatter import convert_file\n",
    "\n",
    "folder_path = '../data_scenic/NHTSA_Scenarios'\n",
    "output_dir = '../data_json/formatted'\n",
    "\n",
    "for _, dirs, _ in os.walk(folder_path):\n",
    "    for dir in dirs:\n",
    "        temp_path = folder_path + '/' + dir\n",
    "        for file_name in os.listdir(temp_path):\n",
    "            # Full path to the file\n",
    "            file_path = os.path.join(temp_path, file_name)\n",
    "            # Test if the file is a .scenic file\n",
    "            if file_name.endswith('.scenic') and os.path.isfile(file_path):\n",
    "                convert_file(file_path, output_dir)\n",
    "\n",
    "examples = []\n",
    "\n",
    "formatted_dir = '../data_json/formatted'\n",
    "\n",
    "for file_name in os.listdir(formatted_dir):\n",
    "    file_path = os.path.join(formatted_dir, file_name) \n",
    "    with open(file_path, \"r\") as f:\n",
    "        scenic_doc = json.load(f) # Load the input from the JSON file\n",
    "        del scenic_doc['has_docstring']\n",
    "        del scenic_doc['name']\n",
    "        for key in scenic_doc.keys():\n",
    "            if scenic_doc[key] == None:\n",
    "                scenic_doc[key] = \"\"\n",
    "        examples.append(scenic_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt message information/Input message information\n",
    "input_message_str = \"\"\"\n",
    "Vehicle is changing lanes in an\n",
    "urban area at a non-junction; and then encroaches into another\n",
    "vehicle traveling in the same direction.\n",
    "\"\"\"\n",
    "\n",
    "input_message = {\"docstring\": input_message_str}\n",
    "input_message_len = num_tokens_from_messages([input_message], model_name, include_final_response_prefix=False)\n",
    "# input_message_len = num_tokens_from_messages([input_message], include_final_response_prefix=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use LangChain Few Shots Examples features to train our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /home/loring/Documents/Llama-2-7B-Chat-GGUF/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens cache size = 259\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 32/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  3718.38 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.14 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   173.04 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     9.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 4\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Using fallback chat format: llama-2\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     495.84 ms /   333 tokens (    1.49 ms per token,   671.58 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     518.19 ms /   334 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     101.94 ms /   512 tokens (    0.20 ms per token,  5022.56 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     127.97 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      85.50 ms /   512 tokens (    0.17 ms per token,  5988.58 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     121.58 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.66 ms /   512 tokens (    0.17 ms per token,  5907.87 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     124.57 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      83.37 ms /   466 tokens (    0.18 ms per token,  5589.81 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     117.97 ms /   467 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      88.36 ms /   512 tokens (    0.17 ms per token,  5794.61 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     114.45 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      85.78 ms /   512 tokens (    0.17 ms per token,  5968.48 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     123.89 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.80 ms /   512 tokens (    0.17 ms per token,  5898.55 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     168.72 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.35 ms /   512 tokens (    0.17 ms per token,  5929.22 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     112.26 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      67.28 ms /   258 tokens (    0.26 ms per token,  3834.95 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =      79.80 ms /   259 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      78.79 ms /   402 tokens (    0.20 ms per token,  5102.24 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =      98.93 ms /   403 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      85.58 ms /   512 tokens (    0.17 ms per token,  5982.64 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     111.56 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.10 ms /   512 tokens (    0.17 ms per token,  5946.85 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     124.40 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      87.16 ms /   512 tokens (    0.17 ms per token,  5874.32 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     124.74 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.93 ms /   512 tokens (    0.17 ms per token,  5889.93 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     113.03 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.50 ms /   512 tokens (    0.17 ms per token,  5918.80 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     112.34 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      89.86 ms /   512 tokens (    0.18 ms per token,  5697.82 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     127.58 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.78 ms /   512 tokens (    0.17 ms per token,  5900.11 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     123.81 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      87.17 ms /   512 tokens (    0.17 ms per token,  5873.72 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     112.16 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.12 ms /   512 tokens (    0.17 ms per token,  5945.19 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     111.29 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.84 ms /   512 tokens (    0.17 ms per token,  5895.90 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     122.33 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.66 ms /   512 tokens (    0.17 ms per token,  5908.08 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     122.67 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.76 ms /   512 tokens (    0.17 ms per token,  5901.13 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     122.44 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.56 ms /   500 tokens (    0.17 ms per token,  5776.61 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     110.81 ms /   501 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.39 ms /   512 tokens (    0.17 ms per token,  5926.75 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     238.54 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      85.97 ms /   512 tokens (    0.17 ms per token,  5955.36 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     122.32 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.53 ms /   512 tokens (    0.17 ms per token,  5916.68 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     122.16 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      80.60 ms /   400 tokens (    0.20 ms per token,  4962.53 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     100.21 ms /   401 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.43 ms /   512 tokens (    0.17 ms per token,  5923.73 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     121.91 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.11 ms /   512 tokens (    0.17 ms per token,  5946.23 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     121.78 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      75.58 ms /   351 tokens (    0.22 ms per token,  4644.21 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =      93.21 ms /   352 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      87.03 ms /   512 tokens (    0.17 ms per token,  5883.16 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     127.56 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      87.95 ms /   512 tokens (    0.17 ms per token,  5821.42 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     124.23 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      87.30 ms /   512 tokens (    0.17 ms per token,  5864.63 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     112.57 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.67 ms /   512 tokens (    0.17 ms per token,  5907.33 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     122.75 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.59 ms /   512 tokens (    0.17 ms per token,  5912.58 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     122.43 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      87.36 ms /   512 tokens (    0.17 ms per token,  5860.87 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     124.50 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      87.33 ms /   512 tokens (    0.17 ms per token,  5862.68 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     113.05 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.29 ms /   502 tokens (    0.17 ms per token,  5817.73 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     121.54 ms /   503 tokens\n"
     ]
    }
   ],
   "source": [
    "# Load the scenario examples\n",
    "to_vectorize = [\" \".join(example.values()) for example in examples]\n",
    "\n",
    "embedding=LlamaCppEmbeddings(\n",
    "        model_path=\"/home/loring/Documents/Llama-2-7B-Chat-GGUF/llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "        n_gpu_layers=32,\n",
    "        n_batch=512\n",
    "    )\n",
    "\n",
    "example_vs = Chroma.from_texts(\n",
    "    embedding=embedding,\n",
    "    texts=to_vectorize,\n",
    "    metadatas=examples,\n",
    "    collection_name=\"example\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      88.48 ms /   512 tokens (    0.17 ms per token,  5786.29 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     122.62 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      87.48 ms /   512 tokens (    0.17 ms per token,  5852.90 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     133.35 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      87.22 ms /   512 tokens (    0.17 ms per token,  5869.94 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     111.27 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.91 ms /   512 tokens (    0.17 ms per token,  5891.22 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     123.26 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      87.01 ms /   512 tokens (    0.17 ms per token,  5884.31 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     120.68 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      85.78 ms /   512 tokens (    0.17 ms per token,  5968.83 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     120.04 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      87.06 ms /   512 tokens (    0.17 ms per token,  5880.66 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     113.46 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      87.61 ms /   512 tokens (    0.17 ms per token,  5843.75 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     133.34 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.11 ms /   512 tokens (    0.17 ms per token,  5945.61 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     130.18 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.21 ms /   512 tokens (    0.17 ms per token,  5938.85 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     125.25 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.84 ms /   512 tokens (    0.17 ms per token,  5895.76 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     125.02 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.38 ms /   512 tokens (    0.17 ms per token,  5927.64 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     120.68 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      85.77 ms /   512 tokens (    0.17 ms per token,  5969.38 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     208.83 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =      13.56 ms /     1 runs   (   13.56 ms per token,    73.75 tokens per second)\n",
      "llama_print_timings:       total time =      13.20 ms /     1 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =      12.98 ms /     1 runs   (   12.98 ms per token,    77.04 tokens per second)\n",
      "llama_print_timings:       total time =      12.72 ms /     1 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =      13.02 ms /     1 runs   (   13.02 ms per token,    76.83 tokens per second)\n",
      "llama_print_timings:       total time =      13.34 ms /     1 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =      12.85 ms /     1 runs   (   12.85 ms per token,    77.81 tokens per second)\n",
      "llama_print_timings:       total time =      12.79 ms /     1 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =      12.81 ms /     1 runs   (   12.81 ms per token,    78.06 tokens per second)\n",
      "llama_print_timings:       total time =      12.74 ms /     1 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =      12.85 ms /     1 runs   (   12.85 ms per token,    77.82 tokens per second)\n",
      "llama_print_timings:       total time =      12.64 ms /     1 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =      17.44 ms /     1 runs   (   17.44 ms per token,    57.34 tokens per second)\n",
      "llama_print_timings:       total time =      17.24 ms /     1 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      89.03 ms /   512 tokens (    0.17 ms per token,  5750.61 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     128.29 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =      13.06 ms /     1 runs   (   13.06 ms per token,    76.56 tokens per second)\n",
      "llama_print_timings:       total time =      13.24 ms /     1 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =      17.53 ms /     1 runs   (   17.53 ms per token,    57.03 tokens per second)\n",
      "llama_print_timings:       total time =      17.57 ms /     1 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.39 ms /   512 tokens (    0.17 ms per token,  5926.75 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     141.02 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.05 ms /   512 tokens (    0.17 ms per token,  5949.89 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     122.34 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      87.06 ms /   512 tokens (    0.17 ms per token,  5881.27 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     121.27 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =      20.32 ms /     1 runs   (   20.32 ms per token,    49.21 tokens per second)\n",
      "llama_print_timings:       total time =      20.16 ms /     1 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =      18.17 ms /     1 runs   (   18.17 ms per token,    55.03 tokens per second)\n",
      "llama_print_timings:       total time =      18.22 ms /     1 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =      18.27 ms /     1 runs   (   18.27 ms per token,    54.75 tokens per second)\n",
      "llama_print_timings:       total time =      17.98 ms /     1 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =      17.61 ms /     1 runs   (   17.61 ms per token,    56.78 tokens per second)\n",
      "llama_print_timings:       total time =      17.28 ms /     1 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =      18.33 ms /     1 runs   (   18.33 ms per token,    54.56 tokens per second)\n",
      "llama_print_timings:       total time =      18.69 ms /     1 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =      18.71 ms /     1 runs   (   18.71 ms per token,    53.46 tokens per second)\n",
      "llama_print_timings:       total time =      18.39 ms /     1 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =      18.26 ms /     1 runs   (   18.26 ms per token,    54.77 tokens per second)\n",
      "llama_print_timings:       total time =      18.20 ms /     1 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =      13.92 ms /     1 runs   (   13.92 ms per token,    71.85 tokens per second)\n",
      "llama_print_timings:       total time =      14.11 ms /     1 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =      13.48 ms /     1 runs   (   13.48 ms per token,    74.18 tokens per second)\n",
      "llama_print_timings:       total time =      13.01 ms /     1 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =      13.12 ms /     1 runs   (   13.12 ms per token,    76.24 tokens per second)\n",
      "llama_print_timings:       total time =      13.34 ms /     1 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.77 ms /   512 tokens (    0.17 ms per token,  5900.93 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     119.23 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.17 ms /   512 tokens (    0.17 ms per token,  5941.47 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     111.32 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.53 ms /   512 tokens (    0.17 ms per token,  5916.89 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     122.40 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      85.58 ms /   512 tokens (    0.17 ms per token,  5982.71 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     119.65 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      87.42 ms /   512 tokens (    0.17 ms per token,  5856.92 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     123.41 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.79 ms /   512 tokens (    0.17 ms per token,  5899.23 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     111.99 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      87.52 ms /   512 tokens (    0.17 ms per token,  5849.89 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     124.48 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      83.82 ms /   471 tokens (    0.18 ms per token,  5619.25 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     115.63 ms /   472 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.56 ms /   512 tokens (    0.17 ms per token,  5915.18 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     117.15 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.64 ms /   512 tokens (    0.17 ms per token,  5909.44 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     123.09 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      87.12 ms /   512 tokens (    0.17 ms per token,  5876.61 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     133.22 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.60 ms /   512 tokens (    0.17 ms per token,  5911.97 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     254.96 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.70 ms /   512 tokens (    0.17 ms per token,  5905.28 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     113.07 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.06 ms /   512 tokens (    0.17 ms per token,  5949.68 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     125.42 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.22 ms /   512 tokens (    0.17 ms per token,  5938.09 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     123.52 ms /   513 tokens\n",
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      85.38 ms /   512 tokens (    0.17 ms per token,  5996.44 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     121.53 ms /   513 tokens\n"
     ]
    }
   ],
   "source": [
    "# Load the reference document as knowledge database\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "source=\"../database\"\n",
    "source_path = \"../document/output-1.json\"\n",
    "CHUNK_SIZE=1000\n",
    "CHUNK_OVERLAP=20\n",
    "\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    metadata[\"title\"] = record.get(\"title\")\n",
    "    metadata[\"content\"] = record.get(\"html\")\n",
    "    return metadata\n",
    "\n",
    "def load_json():\n",
    "    loader = JSONLoader(\n",
    "        file_path=source_path,\n",
    "        jq_schema=\".[]\",\n",
    "        content_key=\"html\",\n",
    "        metadata_func=metadata_func,\n",
    "    )\n",
    "    loaded_json = loader.load()\n",
    "    return loaded_json\n",
    "\n",
    "def split_documents(loaded_docs):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "    )\n",
    "    chunked_docs = splitter.split_documents(loaded_docs)\n",
    "    return chunked_docs\n",
    "\n",
    "chunk_docs = load_json()   \n",
    "# chunk_docs = split_documents(docs)\n",
    "\n",
    "docs_vs = Chroma.from_documents(\n",
    "    documents=chunk_docs,\n",
    "    embedding=embedding,\n",
    "    collection_name=\"docs\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"latest\\n\\nINTRODUCTION\\n\\nGetting Started with Scenic\\nNotes on Installing Scenic\\nWhat’s New in Scenic\\n\\nTUTORIALS\\n\\nScenic Fundamentals\\nDynamic Scenarios\\nComposing Scenarios\\n\\nLANGUAGE AND TOOL REFERENCE\\n\\nSyntax Guide\\nLanguage Reference\\nCommand-Line Options\\nUsing Scenic Programmatically\\nDeveloping Scenic\\nScenic Internals\\nHow Scenic is Compiled\\nGuide to the Scenic Parser & Compiler\\nScenic Grammar\\nScenic Modules\\nscenic.core\\nscenic.domains\\nscenic.domains.driving\\nscenic.formats\\nscenic.simulators\\nscenic.syntax\\n\\nLIBRARIES AND SIMULATORS\\n\\nScenic Libraries\\nSupported Simulators\\nInterfacing to New Simulators\\n\\nGENERAL INFORMATION\\n\\nPublications Using Scenic\\nCredits\\n Scenic Internals scenic.domains scenic.domains.driving scenic.domains.driving.actions\\n Edit on GitHub\\nscenic.domains.driving.actions\\uf0c1\\n\\nActions for dynamic agents in the driving domain.\\n\\nThese actions are automatically imported when using the driving domain.\\n\\nThe RegulatedControlAction is based on code from the CARLA project, licensed under the following terms:\\n\\nCopyright (c) 2018-2020 CVC.\\n\\nThis work is licensed under the terms of the MIT license. For a copy, see <https://opensource.org/licenses/MIT>.\\n\\nSummary of Module Members\\uf0c1\\n\\nClasses\\n\\nOffsetAction\\n\\n\\t\\n\\nTeleports actor forward (in direction of its heading) by some offset.\\n\\n\\n\\n\\nRegulatedControlAction\\n\\n\\t\\n\\nRegulated control of throttle, braking, and steering.\\n\\n\\n\\n\\nSetBrakeAction\\n\\n\\t\\n\\nSet the amount of brake.\\n\\n\\n\\n\\nSetHandBrakeAction\\n\\n\\t\\n\\nSet or release the hand brake.\\n\\n\\n\\n\\nSetPositionAction\\n\\n\\t\\n\\nTeleport an agent to the given position.\\n\\n\\n\\n\\nSetReverseAction\\n\\n\\t\\n\\nEngage or release reverse gear.\\n\\n\\n\\n\\nSetSpeedAction\\n\\n\\t\\n\\nSet the speed of an agent (keeping its heading fixed).\\n\\n\\n\\n\\nSetSteerAction\\n\\n\\t\\n\\nSet the steering 'angle'.\\n\\n\\n\\n\\nSetThrottleAction\\n\\n\\t\\n\\nSet the throttle.\\n\\n\\n\\n\\nSetVelocityAction\\n\\n\\t\\n\\nSet the velocity of an agent.\\n\\n\\n\\n\\nSetWalkingDirectionAction\\n\\n\\t\\n\\nSet the walking direction.\\n\\n\\n\\n\\nSetWalkingSpeedAction\\n\\n\\t\\n\\nSet the walking speed.\\n\\n\\n\\n\\nSteeringAction\\n\\n\\t\\n\\nAbstract class for actions usable by agents which can steer.\\n\\n\\n\\n\\nSteers\\n\\n\\t\\n\\nMixin protocol for agents which can steer.\\n\\n\\n\\n\\nWalkingAction\\n\\n\\t\\n\\nAbstract class for actions usable by agents which can walk.\\n\\n\\n\\n\\nWalks\\n\\n\\t\\n\\nMixin protocol for agents which can walk with a given direction and speed.\\n\\nMember Details\\uf0c1\\nclassSteers[source]\\uf0c1\\n\\nMixin protocol for agents which can steer.\\n\\nSpecifically, agents must support throttling, braking, steering, setting the hand brake, and going into reverse.\\n\\nclassWalks[source]\\uf0c1\\n\\nMixin protocol for agents which can walk with a given direction and speed.\\n\\nWe provide a simplistic implementation which directly sets the velocity of the agent. This implementation needs to be explicitly opted-into, since simulators may provide a more sophisticated API that properly animates pedestrians.\\n\\nclassSetPositionAction(pos)[source]\\uf0c1\\n\\nBases: Action\\n\\nTeleport an agent to the given position.\\n\\nParameters\\n:\\n\\npos (Vector) –\\n\\nclassOffsetAction(offset)[source]\\uf0c1\\n\\nBases: Action\\n\\nTeleports actor forward (in direction of its heading) by some offset.\\n\\nParameters\\n:\\n\\noffset (Vector) –\\n\\nclassSetVelocityAction(xVel, yVel, zVel=0)[source]\\uf0c1\\n\\nBases: Action\\n\\nSet the velocity of an agent.\\n\\nParameters\\n:\\n\\nxVel (float) –\\n\\nyVel (float) –\\n\\nzVel (float) –\\n\\nclassSetSpeedAction(speed)[source]\\uf0c1\\n\\nBases: Action\\n\\nSet the speed of an agent (keeping its heading fixed).\\n\\nParameters\\n:\\n\\nspeed (float) –\\n\\nclassSteeringAction[source]\\uf0c1\\n\\nBases: Action\\n\\nAbstract class for actions usable by agents which can steer.\\n\\nSuch agents must implement the Steers protocol.\\n\\nclassSetThrottleAction(throttle)[source]\\uf0c1\\n\\nBases: SteeringAction\\n\\nSet the throttle.\\n\\nParameters\\n:\\n\\nthrottle (float) – Throttle value between 0 and 1.\\n\\nclassSetSteerAction(steer)[source]\\uf0c1\\n\\nBases: SteeringAction\\n\\nSet the steering ‘angle’.\\n\\nParameters\\n:\\n\\nsteer (float) – Steering ‘angle’ between -1 and 1.\\n\\nclassSetBrakeAction(brake)[source]\\uf0c1\\n\\nBases: SteeringAction\\n\\nSet the amount of brake.\\n\\nParameters\\n:\\n\\nbrake (float) – Amount of braking between 0 and 1.\\n\\nclassSetHandBrakeAction(handBrake)[source]\\uf0c1\\n\\nBases: SteeringAction\\n\\nSet or release the hand brake.\\n\\nParameters\\n:\\n\\nhandBrake (bool) – Whether or not the hand brake is set.\\n\\nclassSetReverseAction(reverse)[source]\\uf0c1\\n\\nBases: SteeringAction\\n\\nEngage or release reverse gear.\\n\\nParameters\\n:\\n\\nreverse (bool) – Whether or not the car is in reverse.\\n\\nclassRegulatedControlAction(throttle, steer, past_steer, max_throttle=0.5, max_brake=0.5, max_steer=0.8)[source]\\uf0c1\\n\\nBases: SteeringAction\\n\\nRegulated control of throttle, braking, and steering.\\n\\nControls throttle and braking using one signal that may be positive or negative. Useful with simple controllers that output a single value.\\n\\nParameters\\n:\\n\\nthrottle (float) – Control signal for throttle and braking (will be clamped as below).\\n\\nsteer (float) – Control signal for steering (also clamped).\\n\\npast_steer (float) – Previous steering signal, for regulating abrupt changes.\\n\\nmax_throttle (float) – Maximum value for throttle, when positive.\\n\\nmax_brake (float) – Maximum (absolute) value for throttle, when negative.\\n\\nmax_steer (float) – Maximum absolute value for steer.\\n\\nclassWalkingAction[source]\\uf0c1\\n\\nBases: Action\\n\\nAbstract class for actions usable by agents which can walk.\\n\\nSuch agents must implement the Walks protocol.\\n\\nclassSetWalkingDirectionAction(heading)[source]\\uf0c1\\n\\nBases: WalkingAction\\n\\nSet the walking direction.\\n\\nclassSetWalkingSpeedAction(speed)[source]\\uf0c1\\n\\nBases: WalkingAction\\n\\nSet the walking speed.\\n\\n Previous\\nNext \\n\\n© Copyright 2020-2024, Daniel J. Fremont. Revision d7679fb8.\\n\\nBuilt with Sphinx using a theme provided by Read the Docs.\\n Read the Docs\\nv: latest \" metadata={'source': '/home/loring/Documents/TestingSceGenWithLLM/document/output-1.json', 'seq_num': 1, 'title': 'scenic.domains.driving.actions — Scenic documentation', 'content': \"latest\\n\\nINTRODUCTION\\n\\nGetting Started with Scenic\\nNotes on Installing Scenic\\nWhat’s New in Scenic\\n\\nTUTORIALS\\n\\nScenic Fundamentals\\nDynamic Scenarios\\nComposing Scenarios\\n\\nLANGUAGE AND TOOL REFERENCE\\n\\nSyntax Guide\\nLanguage Reference\\nCommand-Line Options\\nUsing Scenic Programmatically\\nDeveloping Scenic\\nScenic Internals\\nHow Scenic is Compiled\\nGuide to the Scenic Parser & Compiler\\nScenic Grammar\\nScenic Modules\\nscenic.core\\nscenic.domains\\nscenic.domains.driving\\nscenic.formats\\nscenic.simulators\\nscenic.syntax\\n\\nLIBRARIES AND SIMULATORS\\n\\nScenic Libraries\\nSupported Simulators\\nInterfacing to New Simulators\\n\\nGENERAL INFORMATION\\n\\nPublications Using Scenic\\nCredits\\n Scenic Internals scenic.domains scenic.domains.driving scenic.domains.driving.actions\\n Edit on GitHub\\nscenic.domains.driving.actions\\uf0c1\\n\\nActions for dynamic agents in the driving domain.\\n\\nThese actions are automatically imported when using the driving domain.\\n\\nThe RegulatedControlAction is based on code from the CARLA project, licensed under the following terms:\\n\\nCopyright (c) 2018-2020 CVC.\\n\\nThis work is licensed under the terms of the MIT license. For a copy, see <https://opensource.org/licenses/MIT>.\\n\\nSummary of Module Members\\uf0c1\\n\\nClasses\\n\\nOffsetAction\\n\\n\\t\\n\\nTeleports actor forward (in direction of its heading) by some offset.\\n\\n\\n\\n\\nRegulatedControlAction\\n\\n\\t\\n\\nRegulated control of throttle, braking, and steering.\\n\\n\\n\\n\\nSetBrakeAction\\n\\n\\t\\n\\nSet the amount of brake.\\n\\n\\n\\n\\nSetHandBrakeAction\\n\\n\\t\\n\\nSet or release the hand brake.\\n\\n\\n\\n\\nSetPositionAction\\n\\n\\t\\n\\nTeleport an agent to the given position.\\n\\n\\n\\n\\nSetReverseAction\\n\\n\\t\\n\\nEngage or release reverse gear.\\n\\n\\n\\n\\nSetSpeedAction\\n\\n\\t\\n\\nSet the speed of an agent (keeping its heading fixed).\\n\\n\\n\\n\\nSetSteerAction\\n\\n\\t\\n\\nSet the steering 'angle'.\\n\\n\\n\\n\\nSetThrottleAction\\n\\n\\t\\n\\nSet the throttle.\\n\\n\\n\\n\\nSetVelocityAction\\n\\n\\t\\n\\nSet the velocity of an agent.\\n\\n\\n\\n\\nSetWalkingDirectionAction\\n\\n\\t\\n\\nSet the walking direction.\\n\\n\\n\\n\\nSetWalkingSpeedAction\\n\\n\\t\\n\\nSet the walking speed.\\n\\n\\n\\n\\nSteeringAction\\n\\n\\t\\n\\nAbstract class for actions usable by agents which can steer.\\n\\n\\n\\n\\nSteers\\n\\n\\t\\n\\nMixin protocol for agents which can steer.\\n\\n\\n\\n\\nWalkingAction\\n\\n\\t\\n\\nAbstract class for actions usable by agents which can walk.\\n\\n\\n\\n\\nWalks\\n\\n\\t\\n\\nMixin protocol for agents which can walk with a given direction and speed.\\n\\nMember Details\\uf0c1\\nclassSteers[source]\\uf0c1\\n\\nMixin protocol for agents which can steer.\\n\\nSpecifically, agents must support throttling, braking, steering, setting the hand brake, and going into reverse.\\n\\nclassWalks[source]\\uf0c1\\n\\nMixin protocol for agents which can walk with a given direction and speed.\\n\\nWe provide a simplistic implementation which directly sets the velocity of the agent. This implementation needs to be explicitly opted-into, since simulators may provide a more sophisticated API that properly animates pedestrians.\\n\\nclassSetPositionAction(pos)[source]\\uf0c1\\n\\nBases: Action\\n\\nTeleport an agent to the given position.\\n\\nParameters\\n:\\n\\npos (Vector) –\\n\\nclassOffsetAction(offset)[source]\\uf0c1\\n\\nBases: Action\\n\\nTeleports actor forward (in direction of its heading) by some offset.\\n\\nParameters\\n:\\n\\noffset (Vector) –\\n\\nclassSetVelocityAction(xVel, yVel, zVel=0)[source]\\uf0c1\\n\\nBases: Action\\n\\nSet the velocity of an agent.\\n\\nParameters\\n:\\n\\nxVel (float) –\\n\\nyVel (float) –\\n\\nzVel (float) –\\n\\nclassSetSpeedAction(speed)[source]\\uf0c1\\n\\nBases: Action\\n\\nSet the speed of an agent (keeping its heading fixed).\\n\\nParameters\\n:\\n\\nspeed (float) –\\n\\nclassSteeringAction[source]\\uf0c1\\n\\nBases: Action\\n\\nAbstract class for actions usable by agents which can steer.\\n\\nSuch agents must implement the Steers protocol.\\n\\nclassSetThrottleAction(throttle)[source]\\uf0c1\\n\\nBases: SteeringAction\\n\\nSet the throttle.\\n\\nParameters\\n:\\n\\nthrottle (float) – Throttle value between 0 and 1.\\n\\nclassSetSteerAction(steer)[source]\\uf0c1\\n\\nBases: SteeringAction\\n\\nSet the steering ‘angle’.\\n\\nParameters\\n:\\n\\nsteer (float) – Steering ‘angle’ between -1 and 1.\\n\\nclassSetBrakeAction(brake)[source]\\uf0c1\\n\\nBases: SteeringAction\\n\\nSet the amount of brake.\\n\\nParameters\\n:\\n\\nbrake (float) – Amount of braking between 0 and 1.\\n\\nclassSetHandBrakeAction(handBrake)[source]\\uf0c1\\n\\nBases: SteeringAction\\n\\nSet or release the hand brake.\\n\\nParameters\\n:\\n\\nhandBrake (bool) – Whether or not the hand brake is set.\\n\\nclassSetReverseAction(reverse)[source]\\uf0c1\\n\\nBases: SteeringAction\\n\\nEngage or release reverse gear.\\n\\nParameters\\n:\\n\\nreverse (bool) – Whether or not the car is in reverse.\\n\\nclassRegulatedControlAction(throttle, steer, past_steer, max_throttle=0.5, max_brake=0.5, max_steer=0.8)[source]\\uf0c1\\n\\nBases: SteeringAction\\n\\nRegulated control of throttle, braking, and steering.\\n\\nControls throttle and braking using one signal that may be positive or negative. Useful with simple controllers that output a single value.\\n\\nParameters\\n:\\n\\nthrottle (float) – Control signal for throttle and braking (will be clamped as below).\\n\\nsteer (float) – Control signal for steering (also clamped).\\n\\npast_steer (float) – Previous steering signal, for regulating abrupt changes.\\n\\nmax_throttle (float) – Maximum value for throttle, when positive.\\n\\nmax_brake (float) – Maximum (absolute) value for throttle, when negative.\\n\\nmax_steer (float) – Maximum absolute value for steer.\\n\\nclassWalkingAction[source]\\uf0c1\\n\\nBases: Action\\n\\nAbstract class for actions usable by agents which can walk.\\n\\nSuch agents must implement the Walks protocol.\\n\\nclassSetWalkingDirectionAction(heading)[source]\\uf0c1\\n\\nBases: WalkingAction\\n\\nSet the walking direction.\\n\\nclassSetWalkingSpeedAction(speed)[source]\\uf0c1\\n\\nBases: WalkingAction\\n\\nSet the walking speed.\\n\\n Previous\\nNext \\n\\n© Copyright 2020-2024, Daniel J. Fremont. Revision d7679fb8.\\n\\nBuilt with Sphinx using a theme provided by Read the Docs.\\n Read the Docs\\nv: latest \"}\n"
     ]
    }
   ],
   "source": [
    "print(chunk_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      14.14 ms /     2 tokens (    7.07 ms per token,   141.47 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =      14.24 ms /     3 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': \"latest\\n\\nINTRODUCTION\\n\\nGetting Started with Scenic\\nNotes on Installing Scenic\\nWhat’s New in Scenic\\n\\nTUTORIALS\\n\\nScenic Fundamentals\\nDynamic Scenarios\\nComposing Scenarios\\n\\nLANGUAGE AND TOOL REFERENCE\\n\\nSyntax Guide\\nLanguage Reference\\nCommand-Line Options\\nUsing Scenic Programmatically\\nDeveloping Scenic\\nScenic Internals\\nHow Scenic is Compiled\\nGuide to the Scenic Parser & Compiler\\nScenic Grammar\\nScenic Modules\\nscenic.core\\nscenic.domains\\nscenic.domains.driving\\nscenic.formats\\nscenic.simulators\\nscenic.syntax\\n\\nLIBRARIES AND SIMULATORS\\n\\nScenic Libraries\\nSupported Simulators\\nInterfacing to New Simulators\\n\\nGENERAL INFORMATION\\n\\nPublications Using Scenic\\nCredits\\n Scenic Internals scenic.domains scenic.domains.driving scenic.domains.driving.actions\\n Edit on GitHub\\nscenic.domains.driving.actions\\uf0c1\\n\\nActions for dynamic agents in the driving domain.\\n\\nThese actions are automatically imported when using the driving domain.\\n\\nThe RegulatedControlAction is based on code from the CARLA project, licensed under the following terms:\\n\\nCopyright (c) 2018-2020 CVC.\\n\\nThis work is licensed under the terms of the MIT license. For a copy, see <https://opensource.org/licenses/MIT>.\\n\\nSummary of Module Members\\uf0c1\\n\\nClasses\\n\\nOffsetAction\\n\\n\\t\\n\\nTeleports actor forward (in direction of its heading) by some offset.\\n\\n\\n\\n\\nRegulatedControlAction\\n\\n\\t\\n\\nRegulated control of throttle, braking, and steering.\\n\\n\\n\\n\\nSetBrakeAction\\n\\n\\t\\n\\nSet the amount of brake.\\n\\n\\n\\n\\nSetHandBrakeAction\\n\\n\\t\\n\\nSet or release the hand brake.\\n\\n\\n\\n\\nSetPositionAction\\n\\n\\t\\n\\nTeleport an agent to the given position.\\n\\n\\n\\n\\nSetReverseAction\\n\\n\\t\\n\\nEngage or release reverse gear.\\n\\n\\n\\n\\nSetSpeedAction\\n\\n\\t\\n\\nSet the speed of an agent (keeping its heading fixed).\\n\\n\\n\\n\\nSetSteerAction\\n\\n\\t\\n\\nSet the steering 'angle'.\\n\\n\\n\\n\\nSetThrottleAction\\n\\n\\t\\n\\nSet the throttle.\\n\\n\\n\\n\\nSetVelocityAction\\n\\n\\t\\n\\nSet the velocity of an agent.\\n\\n\\n\\n\\nSetWalkingDirectionAction\\n\\n\\t\\n\\nSet the walking direction.\\n\\n\\n\\n\\nSetWalkingSpeedAction\\n\\n\\t\\n\\nSet the walking speed.\\n\\n\\n\\n\\nSteeringAction\\n\\n\\t\\n\\nAbstract class for actions usable by agents which can steer.\\n\\n\\n\\n\\nSteers\\n\\n\\t\\n\\nMixin protocol for agents which can steer.\\n\\n\\n\\n\\nWalkingAction\\n\\n\\t\\n\\nAbstract class for actions usable by agents which can walk.\\n\\n\\n\\n\\nWalks\\n\\n\\t\\n\\nMixin protocol for agents which can walk with a given direction and speed.\\n\\nMember Details\\uf0c1\\nclassSteers[source]\\uf0c1\\n\\nMixin protocol for agents which can steer.\\n\\nSpecifically, agents must support throttling, braking, steering, setting the hand brake, and going into reverse.\\n\\nclassWalks[source]\\uf0c1\\n\\nMixin protocol for agents which can walk with a given direction and speed.\\n\\nWe provide a simplistic implementation which directly sets the velocity of the agent. This implementation needs to be explicitly opted-into, since simulators may provide a more sophisticated API that properly animates pedestrians.\\n\\nclassSetPositionAction(pos)[source]\\uf0c1\\n\\nBases: Action\\n\\nTeleport an agent to the given position.\\n\\nParameters\\n:\\n\\npos (Vector) –\\n\\nclassOffsetAction(offset)[source]\\uf0c1\\n\\nBases: Action\\n\\nTeleports actor forward (in direction of its heading) by some offset.\\n\\nParameters\\n:\\n\\noffset (Vector) –\\n\\nclassSetVelocityAction(xVel, yVel, zVel=0)[source]\\uf0c1\\n\\nBases: Action\\n\\nSet the velocity of an agent.\\n\\nParameters\\n:\\n\\nxVel (float) –\\n\\nyVel (float) –\\n\\nzVel (float) –\\n\\nclassSetSpeedAction(speed)[source]\\uf0c1\\n\\nBases: Action\\n\\nSet the speed of an agent (keeping its heading fixed).\\n\\nParameters\\n:\\n\\nspeed (float) –\\n\\nclassSteeringAction[source]\\uf0c1\\n\\nBases: Action\\n\\nAbstract class for actions usable by agents which can steer.\\n\\nSuch agents must implement the Steers protocol.\\n\\nclassSetThrottleAction(throttle)[source]\\uf0c1\\n\\nBases: SteeringAction\\n\\nSet the throttle.\\n\\nParameters\\n:\\n\\nthrottle (float) – Throttle value between 0 and 1.\\n\\nclassSetSteerAction(steer)[source]\\uf0c1\\n\\nBases: SteeringAction\\n\\nSet the steering ‘angle’.\\n\\nParameters\\n:\\n\\nsteer (float) – Steering ‘angle’ between -1 and 1.\\n\\nclassSetBrakeAction(brake)[source]\\uf0c1\\n\\nBases: SteeringAction\\n\\nSet the amount of brake.\\n\\nParameters\\n:\\n\\nbrake (float) – Amount of braking between 0 and 1.\\n\\nclassSetHandBrakeAction(handBrake)[source]\\uf0c1\\n\\nBases: SteeringAction\\n\\nSet or release the hand brake.\\n\\nParameters\\n:\\n\\nhandBrake (bool) – Whether or not the hand brake is set.\\n\\nclassSetReverseAction(reverse)[source]\\uf0c1\\n\\nBases: SteeringAction\\n\\nEngage or release reverse gear.\\n\\nParameters\\n:\\n\\nreverse (bool) – Whether or not the car is in reverse.\\n\\nclassRegulatedControlAction(throttle, steer, past_steer, max_throttle=0.5, max_brake=0.5, max_steer=0.8)[source]\\uf0c1\\n\\nBases: SteeringAction\\n\\nRegulated control of throttle, braking, and steering.\\n\\nControls throttle and braking using one signal that may be positive or negative. Useful with simple controllers that output a single value.\\n\\nParameters\\n:\\n\\nthrottle (float) – Control signal for throttle and braking (will be clamped as below).\\n\\nsteer (float) – Control signal for steering (also clamped).\\n\\npast_steer (float) – Previous steering signal, for regulating abrupt changes.\\n\\nmax_throttle (float) – Maximum value for throttle, when positive.\\n\\nmax_brake (float) – Maximum (absolute) value for throttle, when negative.\\n\\nmax_steer (float) – Maximum absolute value for steer.\\n\\nclassWalkingAction[source]\\uf0c1\\n\\nBases: Action\\n\\nAbstract class for actions usable by agents which can walk.\\n\\nSuch agents must implement the Walks protocol.\\n\\nclassSetWalkingDirectionAction(heading)[source]\\uf0c1\\n\\nBases: WalkingAction\\n\\nSet the walking direction.\\n\\nclassSetWalkingSpeedAction(speed)[source]\\uf0c1\\n\\nBases: WalkingAction\\n\\nSet the walking speed.\\n\\n Previous\\nNext \\n\\n© Copyright 2020-2024, Daniel J. Fremont. Revision d7679fb8.\\n\\nBuilt with Sphinx using a theme provided by Read the Docs.\\n Read the Docs\\nv: latest \", 'seq_num': 1, 'source': '/home/loring/Documents/TestingSceGenWithLLM/document/output-1.json', 'title': 'scenic.domains.driving.actions — Scenic documentation'}, {'content': '', 'seq_num': 22, 'source': '/home/loring/Documents/TestingSceGenWithLLM/document/output-1.json', 'title': 'simplest2.jpg (640×400)'}]\n",
      "input_variables=['docstring'] example_selector=SemanticSimilarityExampleSelector(vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7f5ee58efb20>, k=5, example_keys=None, input_keys=None, vectorstore_kwargs=None) example_prompt=PromptTemplate(input_variables=['behaviors', 'constants', 'docstring', 'map_and_model', 'monitors', 'scenario', 'spatial_relations'], template='#Scenario description#\\n{docstring}\\n\\n##Map and Model##\\n{map_and_model}\\n##Constants##\\n{constants}\\n##Moniters##\\n{monitors}\\n##Defining Agent Behaviors##\\n{behaviors}\\n##Spatial Relations##\\n{spatial_relations}\\n##Scenario Specifications##\\n{scenario}\\n') suffix='Generate scenarios based on following description: {docstring}' prefix=\"You are a helpful agent that generates specifications for car driving scenarios in the Scenic language\\nScenic is a domain-specific probabilistic programming language for modeling the environments of cyber-physical systems like robots and autonomous cars. A Scenic program defines a distribution over scenes,configurations of physical objects and agents; sampling from this distribution yields concrete scenes which can be simulated to produce training or testing data. Scenic can also define (probabilistic)policies for dynamic agents, allowing modeling scenarios where agents take actions over time in response to the state of the world.\\nYour task is to generate Scenic scenarios, each according to its corresponding description in English included as a docstring. Please refer all the codes to the given few-shot examples and reference guide syntax. Some self-made APIs are not implemented in Scenic language:\\nYou can reference the instructions in the following documentation: \\nscenic.domains.driving.actions — Scenic documentation \\nlatest\\n\\nINTRODUCTION\\n\\nGetting Started with Scenic\\nNotes on Installing Scenic\\nWhat’s New in Scenic\\n\\nTUTORIALS\\n\\nScenic Fundamentals\\nDynamic Scenarios\\nComposing Scenarios\\n\\nLANGUAGE AND TOOL REFERENCE\\n\\nSyntax Guide\\nLanguage Reference\\nCommand-Line Options\\nUsing Scenic Programmatically\\nDeveloping Scenic\\nScenic Internals\\nHow Scenic is Compiled\\nGuide to the Scenic Parser & Compiler\\nScenic Grammar\\nScenic Modules\\nscenic.core\\nscenic.domains\\nscenic.domains.driving\\nscenic.formats\\nscenic.simulators\\nscenic.syntax\\n\\nLIBRARIES AND SIMULATORS\\n\\nScenic Libraries\\nSupported Simulators\\nInterfacing to New Simulators\\n\\nGENERAL INFORMATION\\n\\nPublications Using Scenic\\nCredits\\n Scenic Internals scenic.domains scenic.domains.driving scenic.domains.driving.actions\\n Edit on GitHub\\nscenic.domains.driving.actions\\uf0c1\\n\\nActions for dynamic agents in the driving domain.\\n\\nThese actions are automatically imported when using the driving domain.\\n\\nThe RegulatedControlAction is based on code from the CARLA project, licensed under the following terms:\\n\\nCopyright (c) 2018-2020 CVC.\\n\\nThis work is licensed under the terms of the MIT license. For a copy, see <https://opensource.org/licenses/MIT>.\\n\\nSummary of Module Members\\uf0c1\\n\\nClasses\\n\\nOffsetAction\\n\\n\\t\\n\\nTeleports actor forward (in direction of its heading) by some offset.\\n\\n\\n\\n\\nRegulatedControlAction\\n\\n\\t\\n\\nRegulated control of throttle, braking, and steering.\\n\\n\\n\\n\\nSetBrakeAction\\n\\n\\t\\n\\nSet the amount of brake.\\n\\n\\n\\n\\nSetHandBrakeAction\\n\\n\\t\\n\\nSet or release the hand brake.\\n\\n\\n\\n\\nSetPositionAction\\n\\n\\t\\n\\nTeleport an agent to the given position.\\n\\n\\n\\n\\nSetReverseAction\\n\\n\\t\\n\\nEngage or release reverse gear.\\n\\n\\n\\n\\nSetSpeedAction\\n\\n\\t\\n\\nSet the speed of an agent (keeping its heading fixed).\\n\\n\\n\\n\\nSetSteerAction\\n\\n\\t\\n\\nSet the steering 'angle'.\\n\\n\\n\\n\\nSetThrottleAction\\n\\n\\t\\n\\nSet the throttle.\\n\\n\\n\\n\\nSetVelocityAction\\n\\n\\t\\n\\nSet the velocity of an agent.\\n\\n\\n\\n\\nSetWalkingDirectionAction\\n\\n\\t\\n\\nSet the walking direction.\\n\\n\\n\\n\\nSetWalkingSpeedAction\\n\\n\\t\\n\\nSet the walking speed.\\n\\n\\n\\n\\nSteeringAction\\n\\n\\t\\n\\nAbstract class for actions usable by agents which can steer.\\n\\n\\n\\n\\nSteers\\n\\n\\t\\n\\nMixin protocol for agents which can steer.\\n\\n\\n\\n\\nWalkingAction\\n\\n\\t\\n\\nAbstract class for actions usable by agents which can walk.\\n\\n\\n\\n\\nWalks\\n\\n\\t\\n\\nMixin protocol for agents which can walk with a given direction and speed.\\n\\nMember Details\\uf0c1\\nclassSteers[source]\\uf0c1\\n\\nMixin protocol for agents which can steer.\\n\\nSpecifically, agents must support throttling, braking, steering, setting the hand brake, and going into reverse.\\n\\nclassWalks[source]\\uf0c1\\n\\nMixin protocol for agents which can walk with a given direction and speed.\\n\\nWe provide a simplistic implementation which directly sets the velocity of the agent. This implementation needs to be explicitly opted-into, since simulators may provide a more sophisticated API that properly animates pedestrians.\\n\\nclassSetPositionAction(pos)[source]\\uf0c1\\n\\nBases: Action\\n\\nTeleport an agent to the given position.\\n\\nParameters\\n:\\n\\npos (Vector) –\\n\\nclassOffsetAction(offset)[source]\\uf0c1\\n\\nBases: Action\\n\\nTeleports actor forward (in direction of its heading) by some offset.\\n\\nParameters\\n:\\n\\noffset (Vector) –\\n\\nclassSetVelocityAction(xVel, yVel, zVel=0)[source]\\uf0c1\\n\\nBases: Action\\n\\nSet the velocity of an agent.\\n\\nParameters\\n:\\n\\nxVel (float) –\\n\\nyVel (float) –\\n\\nzVel (float) –\\n\\nclassSetSpeedAction(speed)[source]\\uf0c1\\n\\nBases: Action\\n\\nSet the speed of an agent (keeping its heading fixed).\\n\\nParameters\\n:\\n\\nspeed (float) –\\n\\nclassSteeringAction[source]\\uf0c1\\n\\nBases: Action\\n\\nAbstract class for actions usable by agents which can steer.\\n\\nSuch agents must implement the Steers protocol.\\n\\nclassSetThrottleAction(throttle)[source]\\uf0c1\\n\\nBases: SteeringAction\\n\\nSet the throttle.\\n\\nParameters\\n:\\n\\nthrottle (float) – Throttle value between 0 and 1.\\n\\nclassSetSteerAction(steer)[source]\\uf0c1\\n\\nBases: SteeringAction\\n\\nSet the steering ‘angle’.\\n\\nParameters\\n:\\n\\nsteer (float) – Steering ‘angle’ between -1 and 1.\\n\\nclassSetBrakeAction(brake)[source]\\uf0c1\\n\\nBases: SteeringAction\\n\\nSet the amount of brake.\\n\\nParameters\\n:\\n\\nbrake (float) – Amount of braking between 0 and 1.\\n\\nclassSetHandBrakeAction(handBrake)[source]\\uf0c1\\n\\nBases: SteeringAction\\n\\nSet or release the hand brake.\\n\\nParameters\\n:\\n\\nhandBrake (bool) – Whether or not the hand brake is set.\\n\\nclassSetReverseAction(reverse)[source]\\uf0c1\\n\\nBases: SteeringAction\\n\\nEngage or release reverse gear.\\n\\nParameters\\n:\\n\\nreverse (bool) – Whether or not the car is in reverse.\\n\\nclassRegulatedControlAction(throttle, steer, past_steer, max_throttle=0.5, max_brake=0.5, max_steer=0.8)[source]\\uf0c1\\n\\nBases: SteeringAction\\n\\nRegulated control of throttle, braking, and steering.\\n\\nControls throttle and braking using one signal that may be positive or negative. Useful with simple controllers that output a single value.\\n\\nParameters\\n:\\n\\nthrottle (float) – Control signal for throttle and braking (will be clamped as below).\\n\\nsteer (float) – Control signal for steering (also clamped).\\n\\npast_steer (float) – Previous steering signal, for regulating abrupt changes.\\n\\nmax_throttle (float) – Maximum value for throttle, when positive.\\n\\nmax_brake (float) – Maximum (absolute) value for throttle, when negative.\\n\\nmax_steer (float) – Maximum absolute value for steer.\\n\\nclassWalkingAction[source]\\uf0c1\\n\\nBases: Action\\n\\nAbstract class for actions usable by agents which can walk.\\n\\nSuch agents must implement the Walks protocol.\\n\\nclassSetWalkingDirectionAction(heading)[source]\\uf0c1\\n\\nBases: WalkingAction\\n\\nSet the walking direction.\\n\\nclassSetWalkingSpeedAction(speed)[source]\\uf0c1\\n\\nBases: WalkingAction\\n\\nSet the walking speed.\\n\\n Previous\\nNext \\n\\n© Copyright 2020-2024, Daniel J. Fremont. Revision d7679fb8.\\n\\nBuilt with Sphinx using a theme provided by Read the Docs.\\n Read the Docs\\nv: latest \\nYou can reference the instructions in the following documentation: \\nsimplest2.jpg (640×400) \\n\\n\"\n"
     ]
    }
   ],
   "source": [
    "examples_prompt = PromptTemplate(\n",
    "    input_variables=[\n",
    "        'docstring', 'map_and_model', 'constants', 'monitors', 'behaviors', 'spatial_relations', 'scenario'\n",
    "    ],\n",
    "    template=\"#Scenario description#\\n{docstring}\\n\\n##Map and Model##\\n{map_and_model}\\n##Constants##\\n{constants}\\n##Moniters##\\n{monitors}\\n##Defining Agent Behaviors##\\n{behaviors}\\n##Spatial Relations##\\n{spatial_relations}\\n##Scenario Specifications##\\n{scenario}\\n\"\n",
    ")\n",
    "\n",
    "doc_prompt = PromptTemplate(\n",
    "    input_variables=['title', 'content'],\n",
    "    template=\"You can reference the instructions in the following documentation: \\n{title} \\n{content}\"\n",
    ")\n",
    " \n",
    "example_selector = SemanticSimilarityExampleSelector(vectorstore=example_vs, k=5)\n",
    "doc_selector = SemanticSimilarityExampleSelector(vectorstore=docs_vs, k=2)\n",
    "selected_reference = doc_selector.select_examples({\"docstring\":\"syntax\"})\n",
    "print(selected_reference)\n",
    "\n",
    "doc_str = system_message_str + \"\\n\"\n",
    "for i in selected_reference:\n",
    "    doc_str += doc_prompt.format(title=i[\"title\"], content=i[\"content\"])\n",
    "    doc_str += \"\\n\"\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    example_prompt=examples_prompt,\n",
    "    example_selector=example_selector,\n",
    "    prefix=doc_str,\n",
    "    suffix=\"Generate scenarios based on following description: {docstring}\",\n",
    "    input_variables=[\"docstring\"],\n",
    ")\n",
    "\n",
    "# Final prompt message is composed of system message, example message and input message\n",
    "final_prompt = few_shot_prompt\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     496.83 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      50.79 ms /    41 tokens (    1.24 ms per token,   807.25 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =      57.86 ms /    42 tokens\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=model, prompt=final_prompt)\n",
    "result = chain.run(input_message_str)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "#Scenario description#\n",
      "\"\"\"\n",
      "TITLE: Lane Change Encroachment\n",
      "AUTHOR: [Your Name], [your.email@example.com]\n",
      "DESCRIPTION: Ego vehicle changes lanes in an urban area at a non-junction and encroaches into another vehicle traveling in the same direction.\n",
      "\"\"\"\n",
      "\n",
      "##Map and Model##\n",
      "\n",
      "param map = localPath('../../../assets/maps/CARLA/Town03.xodr')  # or other CARLA map that definitely works\n",
      "param carla_map = 'Town03'\n",
      "model scenic.simulators.carla.model\n",
      "\n",
      "\n",
      "##Constants##\n",
      "\n",
      "MODEL = 'vehicle.lincoln.mkz_2017'\n",
      "\n",
      "EGO_INIT_DIST = [20, 40]\n",
      "param EGO_SPEED = VerifaiRange(7, 10)\n",
      "param EGO_BRAKE = VerifaiRange(0.5, 1.0)\n",
      "\n",
      "OTHER_INIT_DIST = [0, 20]\n",
      "param OTHER_SPEED = VerifaiRange(7, 10)\n",
      "\n",
      "param SAFETY_DIST = VerifaiRange(10, 20)\n",
      "CRASH_DIST = 5\n",
      "TERM_DIST = 70\n",
      "\n",
      "\n",
      "##Moniters##\n",
      "\n",
      "##Defining Agent Behaviors##\n",
      "\n",
      "behavior EgoBehavior(trajectory):\n",
      "    try:\n",
      "        do FollowTrajectoryBehavior(target_speed=globalParameters.EGO_SPEED, trajectory=trajectory)\n",
      "    interrupt when withinDistanceToAnyObjs(self, globalParameters.SAFETY_DIST):\n",
      "        take SetBrakeAction(globalParameters.EGO_BRAKE)\n",
      "    interrupt when withinDistanceToAnyObjs(self, CRASH_DIST):\n",
      "        terminate\n",
      "\n",
      "\n",
      "##Spatial Relations##\n",
      "\n",
      "road = Uniform(*network.roads)\n",
      "lane1 = Uniform(*road.lanes)\n",
      "lane2 = Uniform(*filter(lambda l: l != lane1, road.lanes))\n",
      "\n",
      "egoInitPoint = new OrientedPoint in lane1.centerline\n",
      "otherInitPoint = new OrientedPoint in lane2.centerline\n",
      "\n",
      "egoTrajectory = [lane1, lane2]\n",
      "otherTrajectory = [lane2]\n",
      "\n",
      "\n",
      "##Scenario Specifications##\n",
      "\n",
      "ego = new Car at egoInitPoint,\n",
      "    with blueprint MODEL,\n",
      "    with behavior EgoBehavior(egoTrajectory)\n",
      "\n",
      "other = new Car at otherInitPoint,\n",
      "    with blueprint MODEL,\n",
      "    with behavior FollowLaneBehavior(target_speed=globalParameters.OTHER_SPEED)\n",
      "\n",
      "require EGO_INIT_DIST[0] <= (distance to road) <= EGO_INIT_DIST[1]\n",
      "require OTHER_INIT_DIST[0] <= (distance from other to road) <= OTHER_INIT_DIST[1]\n",
      "terminate when (distance to egoInitPoint) > TERM_DIST\n",
      "```\n",
      "\n",
      "This Scenic scenario script defines a situation where the ego vehicle changes lanes in an urban area and encroaches into another vehicle's lane. The scenario includes the definition of the map, vehicle models, initial conditions, and behaviors for both the ego and other vehicles. The scenario is terminated when the ego vehicle has moved a certain distance from its initial position.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_string(output: Scenic_output):\n",
    "    scenic_str = '##Map and Model##\\n%s\\n##Constants##\\n%s\\n##Moniters##\\n%s\\n##Defining Agent Behaviors##\\n%s\\n##Spatial Relations##\\n%s\\n##Scenario Specifications##\\n%s\\n##Background Activities##\\n%s\\n' \\\n",
    "             % (output.map_and_model, output.constants, output.monitors, output.behaviors, output.spatial_relations, output.scenario, output.background)\n",
    "    return scenic_str\n",
    "\n",
    "import traceback\n",
    "from scenic.core.errors import ScenicSyntaxError\n",
    "def compile_scenic(scenic_str):\n",
    "    try:\n",
    "        random.seed()\n",
    "        scenario = scenic.scenarioFromString(scenic_str, model='scenic.simulators.carla', mode2D=True)\n",
    "        scene, numIters = scenario.generate(maxIterations=1000)\n",
    "        return scene, numIters\n",
    "    except ScenicSyntaxError as e:\n",
    "        query = str(e) + scenic_str\n",
    "        print(query)\n",
    "        return model.invoke(query)\n",
    "    except Exception as en:\n",
    "        return en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "maps = [\n",
    "    \"Town01\",\n",
    "    \"Town02\",\n",
    "    \"Town03\",\n",
    "    \"Town04\",\n",
    "    \"Town05\",\n",
    "    \"Town06\",\n",
    "    \"Town07\",\n",
    "    \"Town10HD\",\n",
    "]\n",
    "\n",
    "carModels = [\n",
    "    \"vehicle.audi.a2\",\n",
    "    \"vehicle.audi.etron\",\n",
    "    \"vehicle.audi.tt\",\n",
    "    \"vehicle.bmw.grandtourer\",\n",
    "    \"vehicle.chevrolet.impala\",\n",
    "    \"vehicle.citroen.c3\",\n",
    "    \"vehicle.dodge.charger_police\",\n",
    "    \"vehicle.jeep.wrangler_rubicon\",\n",
    "    \"vehicle.lincoln.mkz_2017\",\n",
    "    \"vehicle.mercedes.coupe\",\n",
    "    \"vehicle.mini.cooper_s\",\n",
    "    \"vehicle.ford.mustang\",\n",
    "    \"vehicle.nissan.micra\",\n",
    "    \"vehicle.nissan.patrol\",\n",
    "    \"vehicle.seat.leon\",\n",
    "    \"vehicle.tesla.model3\",\n",
    "    \"vehicle.toyota.prius\",\n",
    "    \"vehicle.volkswagen.t2\",\n",
    "]\n",
    "\n",
    "def modify_output(scenic_code_str):\n",
    "    output_str = scenic_code_str\n",
    "    town_str = random.choice(maps)\n",
    "    map_str = 'param map = localPath(\\'../assets/maps/CARLA/' + town_str + '.xodr\\')' + '\\\\n'\n",
    "    output_str = re.sub(r'^.*localPath.*\\n', map_str, output_str, flags=re.MULTILINE)    # find string like localpath, turn into existing paths(file path should be in Scenic)\n",
    "    output_str = re.sub(r'^.*carla_map.*\\n', 'param carla_map = \\'' + town_str + '\\'\\\\n', output_str, flags=re.MULTILINE)\n",
    "    output_str = re.sub(r'^(\\s*)model scenic.[\\S]*', r'\\1model scenic.simulators.carla.model', output_str, flags=re.MULTILINE)  # use carla as simulator\n",
    "    output_str = re.sub(r'^(\\s*)MODEL = [\\S]*', r'\\1MODEL = ' + '\\'' + random.choice(carModels) + '\\'', output_str, flags=re.MULTILINE)    # modify the model name (pedestrian not included!!!)\n",
    "    output_str = re.sub(r'^.*```.*\\n', '', output_str, flags=re.MULTILINE)\n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Scenario description#\n",
      "\"\"\"\n",
      "TITLE: Lane Change Encroachment\n",
      "AUTHOR: [Your Name], [your.email@example.com]\n",
      "DESCRIPTION: Ego vehicle changes lanes in an urban area at a non-junction and encroaches into another vehicle traveling in the same direction.\n",
      "\"\"\"\n",
      "\n",
      "##Map and Model##\n",
      "\n",
      "param map = localPath('../assets/maps/CARLA/Town01.xodr')\n",
      "param carla_map = 'Town01'\n",
      "model scenic.simulators.carla.model\n",
      "\n",
      "\n",
      "##Constants##\n",
      "\n",
      "MODEL = 'vehicle.toyota.prius'\n",
      "\n",
      "EGO_INIT_DIST = [20, 40]\n",
      "param EGO_SPEED = VerifaiRange(7, 10)\n",
      "param EGO_BRAKE = VerifaiRange(0.5, 1.0)\n",
      "\n",
      "OTHER_INIT_DIST = [0, 20]\n",
      "param OTHER_SPEED = VerifaiRange(7, 10)\n",
      "\n",
      "param SAFETY_DIST = VerifaiRange(10, 20)\n",
      "CRASH_DIST = 5\n",
      "TERM_DIST = 70\n",
      "\n",
      "\n",
      "##Moniters##\n",
      "\n",
      "##Defining Agent Behaviors##\n",
      "\n",
      "behavior EgoBehavior(trajectory):\n",
      "    try:\n",
      "        do FollowTrajectoryBehavior(target_speed=globalParameters.EGO_SPEED, trajectory=trajectory)\n",
      "    interrupt when withinDistanceToAnyObjs(self, globalParameters.SAFETY_DIST):\n",
      "        take SetBrakeAction(globalParameters.EGO_BRAKE)\n",
      "    interrupt when withinDistanceToAnyObjs(self, CRASH_DIST):\n",
      "        terminate\n",
      "\n",
      "\n",
      "##Spatial Relations##\n",
      "\n",
      "road = Uniform(*network.roads)\n",
      "lane1 = Uniform(*road.lanes)\n",
      "lane2 = Uniform(*filter(lambda l: l != lane1, road.lanes))\n",
      "\n",
      "egoInitPoint = new OrientedPoint in lane1.centerline\n",
      "otherInitPoint = new OrientedPoint in lane2.centerline\n",
      "\n",
      "egoTrajectory = [lane1, lane2]\n",
      "otherTrajectory = [lane2]\n",
      "\n",
      "\n",
      "##Scenario Specifications##\n",
      "\n",
      "ego = new Car at egoInitPoint,\n",
      "    with blueprint MODEL,\n",
      "    with behavior EgoBehavior(egoTrajectory)\n",
      "\n",
      "other = new Car at otherInitPoint,\n",
      "    with blueprint MODEL,\n",
      "    with behavior FollowLaneBehavior(target_speed=globalParameters.OTHER_SPEED)\n",
      "\n",
      "require EGO_INIT_DIST[0] <= (distance to road) <= EGO_INIT_DIST[1]\n",
      "require OTHER_INIT_DIST[0] <= (distance from other to road) <= OTHER_INIT_DIST[1]\n",
      "terminate when (distance to egoInitPoint) > TERM_DIST\n",
      "\n",
      "This Scenic scenario script defines a situation where the ego vehicle changes lanes in an urban area and encroaches into another vehicle's lane. The scenario includes the definition of the map, vehicle models, initial conditions, and behaviors for both the ego and other vehicles. The scenario is terminated when the ego vehicle has moved a certain distance from its initial position.\n"
     ]
    }
   ],
   "source": [
    "modified_result = modify_output(result)\n",
    "print(modified_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "output_file_path = '../results/result' + str(i) + '.scenic'\n",
    "try:\n",
    "    with open(output_file_path, 'x') as file:\n",
    "        file.write(modified_result)\n",
    "except FileExistsError:\n",
    "    print(f\"File {output_file_path} already exists\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
