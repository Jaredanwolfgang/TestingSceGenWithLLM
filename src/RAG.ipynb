{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tiktoken\n",
    "import scenic\n",
    "import random\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from OutputParser import Scenic_output\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    ")\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from scenic.simulators.carla import CarlaSimulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Configurations for OpenAI\n",
    "os.environ[\"OPEN_AI_API_KEY\"] = \"sk-gDa1bvL8Ian5Rkdq186bFbDeBf904447B4C707409e660dB6\"\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "model = ChatOpenAI(\n",
    "    model_name=model_name,\n",
    "    max_tokens=2048,\n",
    "    temperature=0,\n",
    "    openai_api_key=os.environ[\"OPEN_AI_API_KEY\"],\n",
    "    openai_api_base=\"https://apikeyplus.com/v1\"\n",
    ")\n",
    "structured_model = model.with_structured_output(Scenic_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful functions\n",
    "###############################################\n",
    "# Calculate the number of tokens in a message.#\n",
    "###############################################\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\", include_final_response_prefix=True):\n",
    "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        #print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if \"gpt-3.5\" in model:\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted                                                                                                                                                                                             \n",
    "    elif \"gpt-4\" in model:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md \\\n",
    "        for information on how messages are converted to tokens.\"\"\")\n",
    "    \n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    if include_final_response_prefix:\n",
    "        num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens\n",
    "\n",
    "################################################\n",
    "# Convert the example to chat messages         #\n",
    "################################################\n",
    "# Example are in json formats and messages is in the format of the chat\n",
    "def example_to_chat_messages(example):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"name\": \"example_user\", \n",
    "         \"content\": '\"\"\" Scenario description\\n%s\\n\"\"\"' % example['docstring']},\n",
    "        {\"role\": \"system\", \"name\": \"example_assistant\", \n",
    "         \"content\": '```##Map and Model##\\n%s\\n##Constants##\\n%s\\n##Moniters##\\n%s\\n##Defining Agent Behaviors##\\n%s\\n##Spatial Relations##\\n%s\\n##Scenario Specifications##\\n%s\\n##Background Activities##\\n%s\\n```' \\\n",
    "             % (example['map_and_model'], example['constants'], example['monitors'], example['behaviors'], example['spatial_relations'], example['scenario'], example['background'])}\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "################################################\n",
    "# Get the number of tokens for the example     #\n",
    "################################################\n",
    "# We need to construct the chat messages from the example and then calculate the number of tokens\n",
    "def get_example_num_tokens(example):\n",
    "    return num_tokens_from_messages(example_to_chat_messages(example), model_name, \n",
    "                                    include_final_response_prefix=False)\n",
    "\n",
    "################################################\n",
    "# Convert the input to chat messages           #\n",
    "################################################\n",
    "# Input is a string or a dictionary with docstring\n",
    "# If the input is a sole string, then we should directly add it to Scenario Description\n",
    "# If the input is a dictionary, then we should extract the docstring from it\n",
    "def input_to_chat_messages(example):\n",
    "    docstring = example if (isinstance(example, str)) else example['docstring']\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": '\"\"\" Scenario description\\n%s\\n\"\"\"' % docstring}\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "################################################\n",
    "# Get the number of tokens for the input       #\n",
    "################################################\n",
    "# We need to construct the chat messages from the input and then calculate the number of tokens\n",
    "def get_input_num_tokens(example, include_final_response_prefix=True):\n",
    "    return num_tokens_from_messages(input_to_chat_messages(example), model_name, \n",
    "                                    include_final_response_prefix=include_final_response_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure a formatted request in LLM should contain the following fields:\n",
    "- _System Message_\n",
    "    - This field will determine the role of the message in the system. It will be used to determine the type of message that is being sent.\n",
    "- _Example Message_\n",
    "    - This field will contain the format of the message that is being sent.\n",
    "- _Prompt Message/Input Message_\n",
    "    - This field will contain the prompt message that is being sent to the model. Based on the prompt message, the model will modify some of the field values in the Example Message and generate new scenarios.\n",
    "- _Response Message_\n",
    "    - This field will contain the response message that is generated by the model. This message will be generated based on the Example Message and Prompt Message.\n",
    "- _RAG Status_\n",
    "    - This field will contain the RAG status of the message. This will be used to determine the quality of the response generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\n"
     ]
    }
   ],
   "source": [
    "# System message information\n",
    "system_message_str=\"\"\"You are a helpful agent that generates specifications for car driving scenarios in the Scenic language\n",
    "Scenic is a domain-specific probabilistic programming language for modeling the environments of cyber-physical systems like robots and autonomous cars. A Scenic program defines a distribution over scenes,\\\n",
    "    configurations of physical objects and agents; sampling from this distribution yields concrete scenes which can be simulated to produce training or testing data. Scenic can also define (probabilistic)\\\n",
    "    policies for dynamic agents, allowing modeling scenarios where agents take actions over time in response to the state of the world.\n",
    "\n",
    "Your task is to generate Scenic scenarios, each according to its corresponding description in English included as a docstring. Write each scenario in a separate code box. Follow the examples below:\"\"\"\n",
    "\n",
    "system_role = \"system\" if model_name.startswith(\"gpt-4\") else \"user\"\n",
    "system_message = {\"role\": system_role, \"content\": system_message_str}\n",
    "system_message_len = num_tokens_from_messages([system_message], model_name, include_final_response_prefix=False)\n",
    "\n",
    "print(system_message_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example message information\n",
    "import os\n",
    "from JsonFormatter import convert_file\n",
    "\n",
    "folder_path = '../data_scenic/NHTSA_Scenarios'\n",
    "output_dir = '../data_json/formatted'\n",
    "\n",
    "for _, dirs, _ in os.walk(folder_path):\n",
    "    for dir in dirs:\n",
    "        temp_path = folder_path + '/' + dir\n",
    "        for file_name in os.listdir(temp_path):\n",
    "            # 拼接文件的完整路径\n",
    "            file_path = os.path.join(temp_path, file_name)\n",
    "            # 检查文件是否是.json文件\n",
    "            if file_name.endswith('.scenic') and os.path.isfile(file_path):\n",
    "                convert_file(file_path, output_dir)\n",
    "\n",
    "examples = []\n",
    "\n",
    "formatted_dir = '../data_json/formatted'\n",
    "\n",
    "for file_name in os.listdir(formatted_dir):\n",
    "    file_path = os.path.join(formatted_dir, file_name) \n",
    "    with open(file_path, \"r\") as f:\n",
    "        scenic_doc = json.load(f) # Load the input from the JSON file\n",
    "        del scenic_doc['has_docstring']\n",
    "        del scenic_doc['name']\n",
    "        examples.append(scenic_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt message information/Input message information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use LangChain Few Shots Examples features to train our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"docstring\", \"{docstring}\"),\n",
    "        (\"map_and_model\", \"{map_and_model}\"),\n",
    "        (\"constants\", \"{constants}\"),\n",
    "        (\"monitors\", \"{monitors}\"),\n",
    "        (\"behaviors\", \"{behaviors}\"),\n",
    "        (\"spatial_relations\", \"{spatial_relations}\"),\n",
    "        (\"scenario\", \"{scenario}\"),\n",
    "        (\"background\", \"{background}\")\n",
    "    ]\n",
    ")\n",
    "to_vectorize = [\" \".join(example.values()) for example in examples]\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=examples_prompt,\n",
    "    example_selector=SemanticSimilarityExampleSelector(\n",
    "        vectorstore=Chroma.from_texts(to_vectorize, OpenAIEmbeddings(), metadatas=examples),\n",
    "        vectorize=True,\n",
    "        k = 2\n",
    "    ),\n",
    "    example_filter=None,\n",
    "    example_replacement=None,\n",
    "    examples=examples,\n",
    ")\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_message_str),\n",
    "        few_shot_prompt\n",
    "    ]\n",
    ")\n",
    "chain = LLMChain(model, final_prompt, max_tokens=2048, temperature=0, top_p=1, top_k=0, max_tokens_per_message=2048)\n",
    "chain.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_string(output: Scenic_output):\n",
    "    scenic_str = '##Map and Model##\\n%s\\n##Constants##\\n%s\\n##Moniters##\\n%s\\n##Defining Agent Behaviors##\\n%s\\n##Spatial Relations##\\n%s\\n##Scenario Specifications##\\n%s\\n##Background Activities##\\n%s\\n' \\\n",
    "             % (output.map_and_model, output.constants, output.monitors, output.behaviors, output.spatial_relations, output.scenario, output.background)\n",
    "    return scenic_str\n",
    "\n",
    "def compile_scenic(scenic_str):\n",
    "    random.seed()\n",
    "    scenario = scenic.scenarioFromString(scenic_str, model='scenic.simulators.carla', mode2D=True)\n",
    "    scene, numIters = scenario.generate(maxIterations=1000)\n",
    "    return scene, numIters\n",
    "\n",
    "def simulate(scene, numIters):\n",
    "    sim = CarlaSimulator()\n",
    "    simulation = sim.simulate(scene, maxIterations=numIters)\n",
    "    if simulation:\n",
    "        result = simulation.result\n",
    "        for i, state in enumerate(result.trajectory):\n",
    "            egoPos, parkedCarPos = state\n",
    "            print(f'Time step {i}: ego at {egoPos}; parked car at {parkedCarPos}')\n",
    "        return True\n",
    "    else:\n",
    "        print('Simulation failed')\n",
    "        return False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
